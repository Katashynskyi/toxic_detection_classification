{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "* Preparing the ground\n",
    "    * Importing Libs and Datasets\n",
    "    * Data check and preprocessing\n",
    "* Feature engineering\n",
    "* Baseline model\n",
    "* More models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libs and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pipreqs\n",
      "  Downloading pipreqs-0.4.11-py2.py3-none-any.whl (32 kB)\n",
      "Collecting yarg\n",
      "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Collecting docopt\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from yarg->pipreqs) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->yarg->pipreqs) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\anaconda3\\lib\\site-packages (from requests->yarg->pipreqs) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->yarg->pipreqs) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->yarg->pipreqs) (3.2)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=4f6269b816699a56549743141c63a8ecdce113bec13ef2b874c5da9555506d33\n",
      "  Stored in directory: c:\\users\\morea\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built docopt\n",
      "Installing collected packages: yarg, docopt, pipreqs\n",
      "Successfully installed docopt-0.6.2 pipreqs-0.4.11 yarg-0.1.9\n"
     ]
    }
   ],
   "source": [
    "pip install pipreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.svm import LinearSVC#,SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report,roc_auc_score,confusion_matrix\n",
    "import pickle\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/Programming/DB's/Toxic_database/tox_train.csv\")\n",
    "#10 sec loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data check and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heatmap target vs features??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804873    Students defined as EBD are legally just as di...\n",
      "Name: comment_text, dtype: object\n",
      "1770642    Students defined as EBD are legally just as di...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(keep=False,subset=['comment_text'],inplace=True)#Dataset duplicates are removed\n",
    "print(df.comment_text.tail(1))\n",
    "df.reset_index(drop=True,inplace=True)#Dropping empty ID's by resetting indexation. Now the last ID is the same as the number of comments.\n",
    "print(df.comment_text.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataset duplicates are removed.\n",
    "* Dropping empty ID's by resetting indexation. Now the last ID is the same as the number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target_class']=(df['target']>=0.5).map(int)#if more than .5 - than toxic.\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(df['comment_text'], df['target_class'],\n",
    "                                                    stratify= df['target_class'], \n",
    "                                                    test_size=0.20)\n",
    "X_test,X_valid,y_test,y_valid=train_test_split(X_rem, y_rem,\n",
    "                                                    stratify= y_rem, \n",
    "                                                    test_size=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating Rough toxic classification based on 0.5 target threshold to count clean and toxic comments (class imbalance).\n",
    "* Split to train/test/validation (X,y, by='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_inderect_features(df):\n",
    "    df=df.to_frame(name='comment_text')\n",
    "    df.loc[:,'count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "    df.loc[:,'count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    df.loc[:,'count_letters'] = df[\"comment_text\"].apply(lambda x: len(str(x)))\n",
    "    df.loc[:,\"count_punctuations\"] = df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    df.loc[:,\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    df.loc[:,\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    df.loc[:,\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "    df.loc[:,\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: round(np.mean([len(w) for w in str(x).split()]),2))\n",
    "    df.loc[:,'word_unique_percent']=df.loc[:,'count_unique_word']*100/df['count_word']\n",
    "    df.loc[:,'punct_percent']=df.loc[:,'count_punctuations']*100/df['count_word']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(ngram_range=(1,1),max_df=0.8,min_df=10)\n",
    "tfidf_X_train=tfidf.fit_transform(X_train)\n",
    "tfidf_X_test=tfidf.transform(X_test)\n",
    "X_train_w_inderect_f=adding_inderect_features(X_train)\n",
    "X_test_w_inderect_f = adding_inderect_features(X_test)\n",
    "inderect_f=set(list(X_train_w_inderect_f.columns))-set(list(X_train.to_frame(name='comment_text')))\n",
    "scaler_1=MinMaxScaler()# also add Scale, norm\n",
    "scaled_X_train_only_features = scaler_1.fit_transform(X_train_w_inderect_f[inderect_f])# numpy.ndarray 10 normalized features \n",
    "scaled_X_test_only_features = scaler_1.fit_transform(X_test_w_inderect_f[inderect_f])\n",
    "sparce_scaled_X_train_only_features=scipy.sparse.csr_matrix(scaled_X_train_only_features)#convert np matrix to csr matrix \n",
    "# 1416514x10 numpy.ndarray to sparse matrix numpy.float64\n",
    "sparce_scaled_X_test_only_features=scipy.sparse.csr_matrix(scaled_X_test_only_features)#convert np matrix to csr matrix \n",
    "sparce_train=hstack((tfidf_X_train, sparce_scaled_X_train_only_features))\n",
    "sparce_test=hstack((tfidf_X_test, sparce_scaled_X_test_only_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFIDF size: ((1416514, 57263), <class 'scipy.sparse.csr.csr_matrix'>)\n",
      "Train Indirect features size: ((1416514, 10), <class 'scipy.sparse.csr.csr_matrix'>)\n",
      "Train result of TFIDF+Indirect_features: ((1416514, 57273), <class 'scipy.sparse.coo.coo_matrix'>)\n",
      "\n",
      "\n",
      "Test TFIDF size: ((177064, 57263), <class 'scipy.sparse.csr.csr_matrix'>)\n",
      "Test Indirect features size: ((177064, 10), <class 'scipy.sparse.csr.csr_matrix'>)\n",
      "Test result of TFIDF+Indirect_features: ((177064, 57273), <class 'scipy.sparse.coo.coo_matrix'>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train TFIDF size: {tfidf_X_train.shape, type(tfidf_X_train)}')\n",
    "print(f'Train Indirect features size: {sparce_scaled_X_train_only_features.shape, type(sparce_scaled_X_train_only_features)}')\n",
    "print(f'Train result of TFIDF+Indirect_features: {sparce_train.shape, type(sparce_train)}')\n",
    "print(f'\\n\\nTest TFIDF size: {tfidf_X_test.shape, type(tfidf_X_test)}')\n",
    "print(f'Test Indirect features size: {sparce_scaled_X_test_only_features.shape, type(sparce_scaled_X_test_only_features)}')\n",
    "print(f'Test result of TFIDF+Indirect_features: {sparce_test.shape, type(sparce_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true,y_pred):\n",
    "    y_true = y_test\n",
    "    target_names = ['Toxic', 'Not_toxic']\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    \n",
    "    roc_auc_score(y_true=y_true,y_score=y_pred,average='samples')\n",
    "    print(f\"Area Under the Curve score: {round(roc_auc_score(y_true,y_pred),2)} \\\n",
    "    \\n Is this probability of Toxic or not toxic? \\\n",
    "    \\n or it's a probability of 1 (Toxic) class to be toxic?\")\n",
    "    \n",
    "    print('\\n',confusion_matrix(y_true=y_true, y_pred=y_pred))\n",
    "    print('\\n',np.array([['true negatives','false negatives'],['true positives','false positives']] ))\n",
    "    print('\\n',np.array([['correct non-toxic predict','wrong not-toxic prediction'],['correct toxic prediction','wrong toxic prediction']] ))\n",
    "    \n",
    "# y_pred=model.predict(sparce_test)\n",
    "# metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_splitted_df's\n",
    "import scipy.sparse\n",
    "os.makedirs('D:/Programming/Repositories/toxic_detection_classification/Model', exist_ok=True)\n",
    "X_train.to_csv('D:/Programming/Repositories/toxic_detection_classification/Model/X_train.csv',index=False)\n",
    "y_train.to_csv('D:/Programming/Repositories/toxic_detection_classification/Model/y_train.csv',index=False)\n",
    "X_test.to_csv('D:/Programming/Repositories/toxic_detection_classification/Model/X_test.csv',index=False)\n",
    "y_test.to_csv('D:/Programming/Repositories/toxic_detection_classification/Model/y_test.csv',index=False)\n",
    "\n",
    "scipy.sparse.save_npz(\"sparce_train.npz\", sparce_train)#save sparce matrix \n",
    "scipy.sparse.save_npz(\"sparce_test.npz\", sparce_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disc\n",
    "X_train = pd.read_csv(\"X_train.csv\").iloc[:,0]\n",
    "y_train = pd.read_csv(\"y_train.csv\").iloc[:,0]\n",
    "X_test = pd.read_csv(\"X_test.csv\").iloc[:,0]\n",
    "y_test = pd.read_csv(\"y_test.csv\").iloc[:,0]\n",
    "sparce_train = scipy.sparse.load_npz(\"sparce_train.npz\")\n",
    "sparce_test = scipy.sparse.load_npz(\"sparce_test.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(columns=[\"model\", \"description\", \"dataset_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,filename='model.sav'):\n",
    "    \"\"\"save the model to disk\"\"\"\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and save\n",
    "linearSVC = LinearSVC(random_state=0, tol=1e-5)\n",
    "LinSVC_fitted=linearSVC.fit(sparce_train, y_train)#(1416514, 57204)\n",
    "save_model(LinSVC_fitted,'LinearSVC_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Toxic       0.95      0.99      0.97    162921\n",
      "   Not_toxic       0.80      0.45      0.58     14143\n",
      "\n",
      "    accuracy                           0.95    177064\n",
      "   macro avg       0.88      0.72      0.78    177064\n",
      "weighted avg       0.94      0.95      0.94    177064\n",
      "\n",
      "Area Under the Curve score: 0.72     \n",
      " Is this probability of Toxic or not toxic?     \n",
      " or it's a probability of 1 (Toxic) class to be toxic?\n",
      "\n",
      " [[161302   1619]\n",
      " [  7710   6433]]\n",
      "\n",
      " [['true negatives' 'false negatives']\n",
      " ['true positives' 'false positives']]\n",
      "\n",
      " [['correct non-toxic predict' 'wrong not-toxic prediction']\n",
      " ['correct toxic prediction' 'wrong toxic prediction']]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "linearSVC = pickle.load(open('LinearSVC_model.sav', 'rb'))\n",
    "y_pred=linearSVC.predict(sparce_test)\n",
    "metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linearSVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16196/1015521700.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinearSVC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparce_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'linearSVC' is not defined"
     ]
    }
   ],
   "source": [
    "linearSVC.predict(sparce_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* skip SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16196/1406368152.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#fit and save\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mSVC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# processed for 10 h and freezes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# SVC_fitted=SVC.fit(sparce_train, y_train)#(1416514, 57204)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SVC' is not defined"
     ]
    }
   ],
   "source": [
    "#fit and save\n",
    "SVC=SVC(random_state=0,gamma='auto',kernel='linear')# processed for 10 h and freezes\n",
    "# SVC_fitted=SVC.fit(sparce_train, y_train)#(1416514, 57204)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DTC Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6232/3669990426.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mDTC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mDTC_fitted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparce_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDTC_fitted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DTC_model.sav'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'save_model' is not defined"
     ]
    }
   ],
   "source": [
    "#fit and save\n",
    "DTC = DecisionTreeClassifier(random_state=0)\n",
    "DTC_fitted=DTC.fit(sparce_train, y_train)\n",
    "save_model(DTC_fitted,'DTC_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'DTC_model.sav'\n",
    "pickle.dump(DTC_fitted, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Toxic       0.95      0.96      0.96    162921\n",
      "   Not_toxic       0.49      0.45      0.47     14143\n",
      "\n",
      "    accuracy                           0.92    177064\n",
      "   macro avg       0.72      0.71      0.71    177064\n",
      "weighted avg       0.92      0.92      0.92    177064\n",
      "\n",
      "Area Under the Curve score: 0.71     \n",
      " Is this probability of Toxic or not toxic?     \n",
      " or it's a probability of 1 (Toxic) class to be toxic?\n",
      "\n",
      " [[156360   6561]\n",
      " [  7764   6379]]\n",
      "\n",
      " [['true negatives' 'false negatives']\n",
      " ['true positives' 'false positives']]\n",
      "\n",
      " [['correct non-toxic predict' 'wrong not-toxic prediction']\n",
      " ['correct toxic prediction' 'wrong toxic prediction']]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "DTC = pickle.load(open('DTC_model.sav', 'rb'))\n",
    "y_pred=DTC.predict(sparce_test)\n",
    "metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0, 0.5, 0.6666666666666666, 0.9, 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(DTC.predict_proba(sparce_test)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest, xgboost, lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 10000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('D:/Programming/Repositories/toxic_detection_classification/Model', exist_ok=True)\n",
    "df_temp.to_csv('D:/Programming/Repositories/toxic_detection_classification/Model/tox_train_featurefull')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
