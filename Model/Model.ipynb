{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "* Preparing the ground\n",
    "    * Importing Libs and Datasets\n",
    "    * Data check and preprocessing\n",
    "    * TF-IDF\n",
    "* Feature engineering\n",
    "\n",
    "* Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libs and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import plotly.express as px\n",
    "# import seaborn as sns\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/Programming/DB's/Toxic_database/tox_train.csv\")\n",
    "#10 sec loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data check and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heatmap target vs features??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep=False,subset=['comment_text'],inplace=True)#no dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataset duplicates are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804873    Students defined as EBD are legally just as di...\n",
      "Name: comment_text, dtype: object\n",
      "1770642    Students defined as EBD are legally just as di...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.comment_text.tail(1))\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print(df.comment_text.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropping empty ID's by resetting indexation. Now the last ID is the same as the number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target_class']=(df['target']>=0.5).map(int)#if more than .5 - than toxic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating Rough toxic classification based on 0.5 target threshold to count clean and toxic comments (class imbalance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['comment_text'], df['target_class'],\n",
    "                                                    stratify= df['target_class'], \n",
    "                                                    test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split to train/test/validation (X,y, by='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1384825\n",
       "1     120221\n",
       "Name: target_class, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 1384825 false samples & 120221 True ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. X_train - fit_transform\n",
    "2. X_test - transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(ngram_range=(1,1),max_df=0.8,min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_train=tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1505046x58647 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 58154419 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It's 1505046x58647 sparse matrix. It has quite a lot of features, should be about 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_test=tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<265597x58647 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10233698 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It's 265597x58647 sparse matrix. It has same amount of features, again, it should be about 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         -0.711242\n",
       "1         -0.646214\n",
       "2         -0.776270\n",
       "3         -0.754594\n",
       "4         -0.949679\n",
       "             ...   \n",
       "1770638   -0.321072\n",
       "1770639   -0.797946\n",
       "1770640   -0.906327\n",
       "1770641    0.134125\n",
       "1770642    1.629775\n",
       "Name: count_word, Length: 1770643, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = df['count_word'].mean()\n",
    "std = df['count_word'].std()\n",
    "(df['count_word'] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15856/2575296523.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "(x - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### * Reminder * \n",
    "* training pipeline: data->preprocessing->f/e->model training->evaluetion ->save model+features+metrics (training flow )\n",
    "* це симуляція inference pipeline \n",
    "* inference pipeline: data+load model+load features->preprocessing->prediction  (production flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = X_train.to_frame()# To show up, it must be very small matrix, or cutted one.\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It must show ok features from EDA step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word count in each comment:\n",
    "df.loc[:,'count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "#Unique word count\n",
    "df.loc[:,'count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "#Letter count\n",
    "df.loc[:,'count_letters'] = df[\"comment_text\"].apply(lambda x: len(str(x)))# воно рахує з пробілами... так і треба?\n",
    "#punctuation count\n",
    "df.loc[:,\"count_punctuations\"] = df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))# I'll -всередині це ж не пунктуація?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upper case words count\n",
    "df.loc[:,\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "#title case words count\n",
    "df.loc[:,\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "#Number of stopwords\n",
    "df.loc[:,\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "#Average length of the words\n",
    "df.loc[:,\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: round(np.mean([len(w) for w in str(x).split()]),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derived features\n",
    "#Word count percent in each comment:\n",
    "df.loc[:,'word_unique_percent']=df.loc[:,'count_unique_word']*100/df['count_word']\n",
    "#Punct percent in each comment:\n",
    "df.loc[:,'punct_percent']=df.loc[:,'count_punctuations']*100/df['count_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "['id', 'target', 'comment_text', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'target_class', 'count_word', 'count_unique_word', 'count_letters', 'count_punctuations', 'count_words_upper', 'count_words_title', 'count_stopwords', 'mean_word_len', 'word_unique_percent', 'punct_percent']\n"
     ]
    }
   ],
   "source": [
    "# direct_features=list(df.columns).remove('comment_text')\n",
    "print(list(df.columns).remove('comment_text'))\n",
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'target_class', 'count_word', 'count_unique_word', 'count_letters', 'count_punctuations', 'count_words_upper', 'count_words_title', 'count_stopwords', 'mean_word_len', 'word_unique_percent', 'punct_percent']\n"
     ]
    }
   ],
   "source": [
    "direct_features = (list(df.columns))\n",
    "direct_features.remove('comment_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_direct_f=df[direct_features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array + sparse matrix\n",
    "\n",
    "#  train = tfidf_X_train  + x_train_direct_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale norm, minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc = scale.fit_transfrom(train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "svm.fit( train_sc, y_train)\n",
    "pred_sc = svm.predict(test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MAE: 10\n"
     ]
    }
   ],
   "source": [
    "mae=10 \n",
    "print(\"  MAE: %s\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale.inverse(pred_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc, conf_matrix, report - metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp\n",
    "df_temp2=df.loc[:,['id','target','comment_text','target_class']]\n",
    "df_temp=pd.concat([df_temp2, df_temp], axis=1, join='inner')\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('D:/Programming/Repositories/toxic_detection_classification/Model', exist_ok=True)\n",
    "df_temp.to_csv('D:/Programming/Repositories/toxic_detection_classification/Model/tox_train_featurefull')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
